{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question pairs\n",
    "\n",
    "The objective of this project is to build a solution for:\n",
    "\n",
    "https://www.kaggle.com/c/quora-question-pairs/overview\n",
    "\n",
    "where one has to detect if a pair of questions are duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T21:04:55.286475Z",
     "start_time": "2022-03-08T21:04:51.190162Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import *\n",
    "import joblib\n",
    "\n",
    "# use this to train and VALIDATE your solution\n",
    "df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "train_df, valid_df = sklearn.model_selection.train_test_split(df, test_size=0.05, random_state=123)\n",
    "train_df = train_df.loc[~(pd.isnull(train_df.question1) | pd.isnull(train_df.question2)), :]\n",
    "corpus = list(train_df[\"question1\"].unique()) + list(train_df[\"question2\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the sentences\n",
    "\n",
    "First we are going to build word embeddings to represent the content of the sentences. We are going to try some embeddings engineered by us (co-occurrance SVD model) and also the pre-trained word2vec embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a co-occurance SVD embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing coocurrance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468461/468461 [03:40<00:00, 2128.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM decomposition\n"
     ]
    }
   ],
   "source": [
    "cooccurrance_embeddings = cooccurrance_embeddings(corpus, preprocess)\n",
    "cooccurrance_embeddings = cooccurrance_embeddings.fit(n_components=300, store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')\n",
    "model.save('embeddings/word2vec-google-news-300.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "Now, for each of the two previous word-embeddings, we compute the sentence embeddings as the TF-IDF scaled sum of the words it contains. Then we compute the distance between the cosine, manhatan, euclidian and word movers distance between two sentence embeddings, the jaccard distance of the questions and pass them all, together with the embeddings, to a logistic classifier and an XGBoost classifier. We also compute BoW embeddings and pass them to a logistic classifier to be used as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating BoW dataset\n",
      "Done creating word2vec dataset\n",
      "Done creating cooccurrence dataset\n"
     ]
    }
   ],
   "source": [
    "transformer = QuoraBaselineTransformer().fit(train_df)\n",
    "X_train_bow = transformer.transform(train_df)\n",
    "X_valid_bow = transformer.transform(valid_df)\n",
    "joblib.dump(transformer, 'transformers/baseline.joblib')\n",
    "print('Done creating BoW dataset')\n",
    "\n",
    "transformer = QuoraTransformer(embeddings_type=EmbeddingType.WORD2VEC).fit(train_df)\n",
    "X_train_word2vec = transformer.transform(train_df)\n",
    "X_valid_word2vec = transformer.transform(valid_df)\n",
    "joblib.dump(transformer, 'transformers/word2vec.joblib')\n",
    "print('Done creating word2vec dataset')\n",
    "\n",
    "transformer = QuoraTransformer(embeddings_type=EmbeddingType.COOCCURRANCE_SVD).fit(train_df)\n",
    "X_train_cooccurrence = transformer.transform(train_df)\n",
    "X_valid_cooccurrence = transformer.transform(valid_df)\n",
    "joblib.dump(transformer, 'transformers/cooccurrance_svd.joblib')\n",
    "print('Done creating cooccurrence dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost on BoW with distances...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.82     10165\n",
      "           1       0.77      0.47      0.59      6007\n",
      "\n",
      "    accuracy                           0.75     16172\n",
      "   macro avg       0.76      0.69      0.70     16172\n",
      "weighted avg       0.76      0.75      0.74     16172\n",
      "\n",
      "\n",
      "Training xgboost on word2vec with distances...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83     10165\n",
      "           1       0.72      0.71      0.72      6007\n",
      "\n",
      "    accuracy                           0.79     16172\n",
      "   macro avg       0.78      0.77      0.77     16172\n",
      "weighted avg       0.79      0.79      0.79     16172\n",
      "\n",
      "\n",
      "Training xgboost on cooccurrence with distances...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81     10165\n",
      "           1       0.68      0.64      0.66      6007\n",
      "\n",
      "    accuracy                           0.76     16172\n",
      "   macro avg       0.74      0.73      0.73     16172\n",
      "weighted avg       0.75      0.76      0.75     16172\n",
      "\n",
      "\n",
      "Training logistic on BoW with distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volokin/PythonEnvironments/data-science-3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81     10165\n",
      "           1       0.70      0.60      0.65      6007\n",
      "\n",
      "    accuracy                           0.76     16172\n",
      "   macro avg       0.74      0.72      0.73     16172\n",
      "weighted avg       0.75      0.76      0.75     16172\n",
      "\n",
      "\n",
      "Training logistic on word2vec with distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volokin/PythonEnvironments/data-science-3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79     10165\n",
      "           1       0.65      0.57      0.61      6007\n",
      "\n",
      "    accuracy                           0.73     16172\n",
      "   macro avg       0.71      0.69      0.70     16172\n",
      "weighted avg       0.72      0.73      0.72     16172\n",
      "\n",
      "\n",
      "Training logistic on cooccurrence with distances...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.82      0.74     10165\n",
      "           1       0.53      0.34      0.41      6007\n",
      "\n",
      "    accuracy                           0.64     16172\n",
      "   macro avg       0.60      0.58      0.58     16172\n",
      "weighted avg       0.62      0.64      0.62     16172\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volokin/PythonEnvironments/data-science-3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train = train_df['is_duplicate']\n",
    "y_valid = valid_df['is_duplicate']\n",
    "\n",
    "classifiers = {\n",
    "    'xgboost': xgb.XGBClassifier(random_state=123),\n",
    "    'logistic': linear_model.LogisticRegression(random_state=123)\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'BoW': (X_train_bow, X_valid_bow),\n",
    "    'word2vec': (X_train_word2vec, X_valid_word2vec),\n",
    "    'cooccurrence': (X_train_cooccurrence, X_valid_cooccurrence)\n",
    "}\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    for emb_name, data in datasets.items():\n",
    "        print(f'Training {clf_name} on {emb_name} with distances...')\n",
    "        clf.fit(data[0], y_train)\n",
    "        y_hat = clf.predict(data[1])\n",
    "        print(metrics.classification_report(y_valid, y_hat) + '\\n')\n",
    "    \n",
    "        joblib.dump(clf, f'models/{emb_name}_{clf_name}.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
